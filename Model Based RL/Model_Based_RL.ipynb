{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete State and Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 States, 2 Actions\n",
    "\n",
    "transition_mat = [[\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.2, 0.7, 0.1],\n",
    "    [0.3, 0.3, 0.4]\n",
    "],[\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.2, 0.7, 0.1]\n",
    "]]\n",
    "\n",
    "reward = [tfd.Normal(10, 5), tfd.Normal(3, 3), tfd.Normal(2, 1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous State, Discrete Action, deterministic transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reward(state):\n",
    "    return - np.sum(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([1000, 1000, 1000, 1000]) # some warehouse quantities\n",
    "demand_t = np.array([500, 200, 300, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier = np.array([\n",
    "    [200, 200, 200, 200],\n",
    "    [400, 200,300,500],\n",
    "    [700, 100, 200, 200]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_transition(state, action):\n",
    "    return state + supplier[action] - demand_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(s, a, w_t):\n",
    "    \"\"\"\n",
    "    predict the next possible state s_prime\n",
    "    \"\"\"\n",
    "    next_state = np.dot([s,a], w_t)\n",
    "\n",
    "    return s ** w_t + a ** w_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Variable(3.0)\n",
    "a = tf.Variable(2.0)\n",
    "w_t = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = T(s,a,w_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y, w_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R(s,a):\n",
    "    \"\"\"\n",
    "    predicts the next possible reward based on the state that the agent was in and the action\n",
    "    he took\n",
    "    \"\"\"\n",
    "    reward = None\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(s,a):\n",
    "    \"\"\"\n",
    "    calculates the Q Value\n",
    "    \"\"\"\n",
    "    q_val = None\n",
    "\n",
    "    return q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROUNDS = 1000\n",
    "alpha_t = 0.1\n",
    "alpha_r = 0.1\n",
    "alpha_q = 0.1\n",
    "k = 10\n",
    "gamma = 0.9\n",
    "s = state = np.array([1000, 1000, 1000, 1000])\n",
    "action_space = np.array([0,1,2])\n",
    "\n",
    "for round in N_ROUNDS:\n",
    "\n",
    "    s_prime, r = true_transition(s, np.random.choice(action_space))\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        t_sa = T(s,a)\n",
    "        r_sa = R(s,a)\n",
    "    \n",
    "    w_t = w_t - alpha_t*(t_sa-s_prime)*tape.gradient(t_sa, w_t)\n",
    "    w_r = w_r - alpha_r*(r_sa - r)*tape.gradient(r_sa, w_r)\n",
    "\n",
    "    for i in k:\n",
    "        s_hat = None\n",
    "        a_hat = None\n",
    "\n",
    "        with tf.GradientTape() as gt:\n",
    "            q_val = Q(s_hat, a_hat)\n",
    "\n",
    "\n",
    "        delta = R(s_hat, a_hat) + gamma * Q(T(s_hat, a_hat), a_hat_prime)\n",
    "        w_q = w_q - alpha_q*delta*gt.gradient(q_val, w_q)\n",
    "    \n",
    "    s = s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
